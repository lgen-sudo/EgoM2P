# <p align="center">EgoM2P: Egocentric Multimodal Multitask Pretraining </p>

####  <p align="center"> [Gen Li](https://vlg.inf.ethz.ch/team/Gen-Li.html), [Yutong Chen](https://vlg.inf.ethz.ch/team/Yutong-Chen.html), [Yiqian Wu](https://onethousandwu.com/), [Kaifeng Zhao](https://vlg.inf.ethz.ch/team/Kaifeng-Zhao.html), [Marc Pollefeys](https://people.inf.ethz.ch/marc.pollefeys/), [Siyu Tang](https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html)</p>

### <p align="center">[ArXiv]() | [Project Page](https://egom2p.github.io/)

<p align="center">
  <img width="100%" src="assets/teaser.png"/>
</p><p align="center">
  <b>EgoM2P</b>: A large-scale egocentric multimodal and multitask model, pretrained on eight extensive egocentric datasets. It incorporates four modalities—RGB and depth video, gaze dynamics, and camera trajectories—to handle challenging tasks like monocular egocentric depth estimation, camera tracking, gaze estimation, and conditional egocentric video synthesis. For simplicity, we only visualize four frames here.
</p>

## Code will be released soon


## Citation
```

```

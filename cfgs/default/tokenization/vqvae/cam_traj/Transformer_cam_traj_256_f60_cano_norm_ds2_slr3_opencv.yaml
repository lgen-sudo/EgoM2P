run_name: auto

# Architecture
encoder_type: CamTransformer
decoder_type: CamTransformer
num_frames: 60
post_mlp: True # tanh MLP before quantizer

# Quantizer
codebook_size: 256
latent_dim: 32
norm_codes: True
quantizer_type: lucid
coef_ema_dead_code: 32.0
code_replacement_policy: batch_random
commitment_weight: 1.0
quantizer_ema_decay: 0.99
kmeans_init: False

# Losses
loss_fn: mse
dtype_percept: fp32
codebook_weight: 1.0

# Train
dtype: fp32 # fp32, fp16 or bf16
epochs: 200
opt: adamw
opt_betas: [0.9, 0.99]
blr: 0.000025 # base_lr = old value: 4e-5, lr = base_lr * batch_size / 256
warmup_lr: 0.000001 # 1e-6
min_lr: 0.
warmup_epochs: 5
batch_size: 128 # 64 # per GPU
clip_grad: 1.0
model_ema: False
model_ema_decay: 0.9999
model_ema_update_freq: 1
find_unused_params: False
save_ckpt_freq: 1
init_weights: False
temporal_downsampling: 2

# Eval
step_eval: False
epoch_eval: True
eval_freq: 1
eval_metrics_freq: 0 # disable just for now
eval_image_log_freq: 0 # do not log generated video now
num_eval_metrics_samples: 50000 # Number of samples to use for evaluating image metrics during training.
num_logged_images: 100
input_size_eval: 224

# Data
domain: cam
imagenet_default_mean_and_std: False # Normalize to [-1,1]
min_crop_scale: 0.8
data_path: '/capstor/store/cscs/swissai/a03/datasets/holoassist' # Change me
eval_data_path: '/capstor/store/cscs/swissai/a03/datasets/holoassist' # Change me

# Wandb
log_wandb: True # Set to True to log to Weights & Biases
wandb_project: 'egom2p-tokenizers'
wandb_entity: null # Change if needed
wandb_run_name: auto
output_dir: '/iopsstor/scratch/cscs/lgen/output_fix/auto'

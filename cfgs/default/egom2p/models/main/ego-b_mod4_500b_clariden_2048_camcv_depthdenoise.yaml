# Config for DDP

# Arch: SwiGLU No Bias
# Modalities: rgb, depth, cam, gaze
# To be run on 64 GPUs for batch size = 2048
run_name: auto

# Input & output
num_input_tokens: 2048
num_target_tokens: 2048
loss_type: mod

# Architecture
model: egom2p_base_12e_12d_swiglu_nobias
# patch_size: 16
# input_size: 224
dtype: bfloat16

# Train
epochs: -1
total_tokens: 500 # in billions
opt: adamw
blr: 0.0001 # this is base_lr = 1e-4, lr = base_lr * batch_size / 256
min_blr: 0.
warmup_epochs: -1
warmup_tokens: 10 # in billions
batch_size: 4 # 32 x 64 = 2048

# Data
data_config: "cfgs/default/egom2p/data/ego/main/mix_mod4_all2all_2048.yaml"
s3_data_endpoint: "/path/to/endpoint" # Change me
eval_freq: 1
fixed_eval: False
epoch_size: 1_000_000 # Number of samples per "epoch"
clip_grad: 1.

# Saving
save_ckpt_freq: 1
output_dir: '/iopsstor/scratch/cscs/lgen/output_fix/auto'

# Wandb
log_wandb: True # Set to True to log to Weights & Biases
wandb_project: 'egom2p-train'
wandb_entity: null # Change if needed
wandb_run_name: auto
